{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff2b355",
   "metadata": {},
   "source": [
    "# ATS MDA workflow (expanded and optimized)\n",
    "\n",
    "This notebook implements a **step-by-step, auditable workflow** for preparing an **ATS** model run directory for publication as a **Model Data Archive (MDA)** in **ESS-DIVE**.\n",
    "\n",
    "The workflow is intentionally split into two complementary deliverables:\n",
    "\n",
    "1. **A curated copy of your simulation directory** (the *data package directory*) that contains only files you intend to archive.\n",
    "2. Two ESS-DIVE companion tables generated from that curated copy:\n",
    "   - `flmd.csv` — *file-level metadata* (one row per file)\n",
    "   - `dd.csv` — *data dictionary* for key tabular outputs (column names, units, definitions)\n",
    "\n",
    "## What this notebook does (high level)\n",
    "\n",
    "1. **Configuration & validation**\n",
    "   - Read your scratch/work location from the environment (commonly `SCRATCH` on HPC systems).\n",
    "   - Define: (a) the source simulation directory, (b) the destination data package directory.\n",
    "\n",
    "2. **Copy only publishable artifacts**\n",
    "   - Use `rsync` include/exclude rules (fast, preserves structure).\n",
    "   - Optionally fall back to a pure-Python copier if `rsync` is unavailable.\n",
    "\n",
    "3. **Post-copy cleanup**\n",
    "   - Remove bulky or intermediate files that are not needed for publication (e.g., periodic checkpoints, visualization time-series, Paraview `.xmf`).\n",
    "   - Keep final checkpoints (or any file patterns you explicitly choose).\n",
    "\n",
    "4. **Enumerate files and generate `flmd.csv`**\n",
    "   - Walk the curated data package directory.\n",
    "   - Create a file manifest with paths plus a first-pass description field you can refine.\n",
    "\n",
    "5. **Generate `dd.csv` for tabular outputs**\n",
    "   - Locate observation/output CSVs (default: `water_balance*.csv`).\n",
    "   - Parse the header into *parameter names* and *units*.\n",
    "   - Auto-populate *definitions* from ATS input-spec documentation when possible.\n",
    "\n",
    "## What you must still do manually\n",
    "\n",
    "- Review and edit both CSVs (especially descriptions/definitions) to reflect your manuscript and study context.\n",
    "- Add project-level metadata (authors, funding, spatial/temporal coverage, etc.) using the ESS-DIVE submission interface and templates.\n",
    "- Upload the curated package and metadata to ESS-DIVE.\n",
    "\n",
    "## Assumptions and expectations\n",
    "\n",
    "- You have a simulation directory with a stable structure (often including run subfolders like `run0`, `run1`, etc.).\n",
    "- You have sufficient disk quota to create a curated copy.\n",
    "- You are comfortable adjusting a few configuration variables in the **Configuration** section.\n",
    "\n",
    "## Safety and reproducibility notes\n",
    "\n",
    "- This workflow is designed to be *non-destructive to the original simulation directory*.\n",
    "- **Deletion steps only operate inside the destination data package directory**, not the source.\n",
    "- If you enable optional CSV rewriting, the notebook defaults to writing a separate `*_tmp` file rather than overwriting in place.\n",
    "\n",
    "---\n",
    "\n",
    "Proceed top-to-bottom. Each section includes context, rationale, and what to verify before moving on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891f2303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Logging\n",
    "# ----------------------------------------------------------------------------\n",
    "logger = logging.getLogger(\"ats-mda\")\n",
    "if not logger.handlers:\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    )\n",
    "\n",
    "\n",
    "def which(executable: str) -> Optional[str]:\n",
    "    \"\"\"Return the resolved path to an executable or None if not found.\"\"\"\n",
    "    return shutil.which(executable)\n",
    "\n",
    "\n",
    "def run_cmd(cmd: List[str], *, cwd: Optional[Path] = None) -> str:\n",
    "    \"\"\"Run a command safely (no shell), log it, and return stdout.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    subprocess.CalledProcessError\n",
    "        If the command exits non-zero.\n",
    "    \"\"\"\n",
    "    logger.info(\"Running: %s\", \" \".join(cmd))\n",
    "    result = subprocess.run(\n",
    "        cmd,\n",
    "        cwd=str(cwd) if cwd else None,\n",
    "        check=True,\n",
    "        text=True,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "    )\n",
    "    if result.stderr.strip():\n",
    "        # Keep stderr visible but do not treat as failure if exit code was 0.\n",
    "        logger.debug(\"stderr: %s\", result.stderr.strip())\n",
    "    return result.stdout\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def run_cmd_live(cmd: List[str], *, cwd: Optional[Path] = None) -> None:\n",
    "    \"\"\"Run a command and stream stdout/stderr live to the notebook.\n",
    "\n",
    "    This is particularly useful for long-running tools (e.g., rsync) where\n",
    "    users expect to see progress in real time.\n",
    "    \"\"\"\n",
    "    logger.info(\"Running (live): %s\", \" \".join(cmd))\n",
    "    subprocess.run(\n",
    "        cmd,\n",
    "        cwd=str(cwd) if cwd else None,\n",
    "        check=True,\n",
    "        text=True,\n",
    "    )\n",
    "def ensure_dir(path: Path) -> None:\n",
    "    \"\"\"Create a directory if it does not exist.\"\"\"\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def human_bytes(num_bytes: int) -> str:\n",
    "    \"\"\"Format bytes as a human-friendly string.\"\"\"\n",
    "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\"]\n",
    "    size = float(num_bytes)\n",
    "    for u in units:\n",
    "        if size < 1024.0 or u == units[-1]:\n",
    "            return f\"{size:.2f} {u}\"\n",
    "        size /= 1024.0\n",
    "\n",
    "\n",
    "def dir_size_bytes(root: Path) -> int:\n",
    "    \"\"\"Compute total size of all files under root.\"\"\"\n",
    "    total = 0\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file():\n",
    "            total += p.stat().st_size\n",
    "    return total\n",
    "\n",
    "\n",
    "def list_files(root: Path) -> List[Path]:\n",
    "    \"\"\"Return all files under root (sorted), as absolute Paths.\"\"\"\n",
    "    return sorted([p for p in root.rglob(\"*\") if p.is_file()])\n",
    "\n",
    "\n",
    "def rel_manifest_rows(root: Path, files: List[Path]) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Return (filenames, file_paths) for an ESS-DIVE-like manifest.\n",
    "\n",
    "    - File_Name is the basename\n",
    "    - File_Path is a *relative* folder path with a trailing slash\n",
    "    \"\"\"\n",
    "    names: List[str] = []\n",
    "    paths: List[str] = []\n",
    "\n",
    "    for p in files:\n",
    "        rel = p.relative_to(root)\n",
    "        names.append(rel.name)\n",
    "        parent = rel.parent.as_posix()\n",
    "        if parent in (\"\", \".\"):\n",
    "            paths.append(\"./\")\n",
    "        else:\n",
    "            paths.append(f\"./{parent}/\")\n",
    "    return names, paths\n",
    "\n",
    "\n",
    "def normalize_column_name(name: str) -> str:\n",
    "    \"\"\"Normalize a header label to a stable, machine-friendly column name.\n",
    "\n",
    "    This follows the spirit of the original notebook (replace spaces with\n",
    "    underscores), but is more explicit and predictable.\n",
    "    \"\"\"\n",
    "    name = name.strip()\n",
    "    name = re.sub(r\"\\s+\", \"_\", name)\n",
    "    name = re.sub(r\"[^0-9A-Za-z_\\.-]\", \"\", name)\n",
    "    name = re.sub(r\"_+\", \"_\", name)\n",
    "    return name\n",
    "\n",
    "\n",
    "def parse_ats_csv_header_line(line: str) -> Tuple[List[str], List[str]]:\n",
    "    \"\"\"Parse a CSV header of the form: `var [unit], var2 [unit2], ...`.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    parameters : list[str]\n",
    "        Normalized column names.\n",
    "    units : list[str]\n",
    "        Unit strings (\"N/A\" if missing).\n",
    "    \"\"\"\n",
    "    tokens = [t.strip().strip(\"\\ufeff\") for t in line.strip().split(\",\")]\n",
    "\n",
    "    params: List[str] = []\n",
    "    units: List[str] = []\n",
    "\n",
    "    pattern = re.compile(r\"^(.*?)\\s*\\[(.*?)\\]\\s*$\")\n",
    "\n",
    "    for tok in tokens:\n",
    "        m = pattern.match(tok)\n",
    "        if m:\n",
    "            raw_name, raw_unit = m.group(1), m.group(2)\n",
    "            params.append(normalize_column_name(raw_name))\n",
    "            units.append(raw_unit.strip() or \"N/A\")\n",
    "        else:\n",
    "            # No unit bracket detected; keep the full token as the name.\n",
    "            params.append(normalize_column_name(tok))\n",
    "            units.append(\"N/A\")\n",
    "\n",
    "    return params, units\n",
    "\n",
    "\n",
    "def detect_header_line(lines: List[str], *, max_lines: int = 400) -> int:\n",
    "    \"\"\"Heuristically detect a header line index in ATS-style CSV outputs.\n",
    "\n",
    "    Many ATS outputs include a preamble, then a single header line that contains\n",
    "    multiple comma-separated fields, commonly with unit brackets `[ ... ]`.\n",
    "\n",
    "    The heuristic below looks for the first line that:\n",
    "    - contains commas (suggesting CSV), AND\n",
    "    - contains at least one '[' and one ']' (suggesting `name [unit]` tokens)\n",
    "\n",
    "    If your file does not follow this pattern, set a manual override.\n",
    "    \"\"\"\n",
    "    scan = lines[:max_lines]\n",
    "    for i, line in enumerate(scan):\n",
    "        if \",\" in line and \"[\" in line and \"]\" in line:\n",
    "            return i\n",
    "    raise ValueError(\n",
    "        \"Could not auto-detect a header line. \"\n",
    "        \"Set `header_line_hint` explicitly in the configuration section.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def parse_org_table(text: str) -> pd.DataFrame:\n",
    "    \"\"\"Parse an org-mode pipe table into a DataFrame.\n",
    "\n",
    "    This is a lightweight parser that:\n",
    "    - keeps rows starting with '|'\n",
    "    - discards separator rows (those that are mostly dashes/plus)\n",
    "    \"\"\"\n",
    "    rows: List[List[str]] = []\n",
    "    for raw in text.splitlines():\n",
    "        line = raw.strip()\n",
    "        if not line.startswith(\"|\"):\n",
    "            continue\n",
    "        # Skip org separator lines like |-----+-----|\n",
    "        if set(line.replace(\"|\", \"\").strip()) <= set(\"-+\"):\n",
    "            continue\n",
    "        parts = [p.strip() for p in line.strip(\"|\").split(\"|\")]\n",
    "        rows.append(parts)\n",
    "\n",
    "    if not rows:\n",
    "        raise ValueError(\"No org-table rows found.\")\n",
    "\n",
    "    header = rows[0]\n",
    "    data = rows[1:]\n",
    "\n",
    "    df = pd.DataFrame(data, columns=header)\n",
    "    # Drop empty column names if any\n",
    "    df = df.loc[:, [c for c in df.columns if str(c).strip()]]\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_definition_map_from_symbol_table(df_symbol: pd.DataFrame) -> Dict[str, str]:\n",
    "    \"\"\"Best-effort extraction of (variable_root -> description) from ATS symbol table.\"\"\"\n",
    "\n",
    "    cols = [c.lower() for c in df_symbol.columns]\n",
    "\n",
    "    # Try to locate columns by name first.\n",
    "    var_col = None\n",
    "    desc_col = None\n",
    "    for c in df_symbol.columns:\n",
    "        cl = str(c).lower()\n",
    "        if var_col is None and \"variable\" in cl and \"name\" in cl:\n",
    "            var_col = c\n",
    "        if desc_col is None and \"description\" in cl:\n",
    "            desc_col = c\n",
    "\n",
    "    # Fall back to historical positional assumptions.\n",
    "    if var_col is None or desc_col is None:\n",
    "        if df_symbol.shape[1] >= 4:\n",
    "            var_col = df_symbol.columns[1]\n",
    "            desc_col = df_symbol.columns[3]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Unexpected symbol table shape; cannot infer variable/description columns.\"\n",
    "            )\n",
    "\n",
    "    # Build map, removing obvious empties.\n",
    "    out: Dict[str, str] = {}\n",
    "    for v, d in zip(df_symbol[var_col], df_symbol[desc_col]):\n",
    "        v = str(v).strip()\n",
    "        d = str(d).strip()\n",
    "        if v and v.lower() != \"nan\" and d and d.lower() != \"nan\":\n",
    "            out[v] = d\n",
    "    return out\n",
    "\n",
    "\n",
    "def fill_definitions(parameters: List[str], definition_map: Dict[str, str]) -> List[str]:\n",
    "    \"\"\"Fill definitions by substring matching (longest-key-wins).\n",
    "\n",
    "    This mirrors the intent of the original notebook (match symbol-table keys\n",
    "    within column names) but avoids pandas chained assignment and is faster.\n",
    "    \"\"\"\n",
    "    keys = sorted(definition_map.keys(), key=len, reverse=True)\n",
    "    defs: List[str] = []\n",
    "    for p in parameters:\n",
    "        matched = \"\"\n",
    "        for k in keys:\n",
    "            if k in p:\n",
    "                matched = definition_map[k]\n",
    "                break\n",
    "        defs.append(matched)\n",
    "    return defs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076f7d83",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "This is the only section you should need to edit for a typical project.\n",
    "\n",
    "### Key paths\n",
    "\n",
    "- **`SCRATCH_ROOT`**: Where you want the curated data package to live.\n",
    "  - On many HPC systems this is provided as an environment variable named `SCRATCH`.\n",
    "  - If `SCRATCH` is not set, the notebook falls back to the current working directory.\n",
    "\n",
    "- **`SIMULATION_DIR`**: The *source* directory containing your ATS run outputs.\n",
    "\n",
    "- **`DATA_PKG_DIR`**: The *destination* directory that becomes your MDA payload.\n",
    "  - The notebook copies files into this directory and may delete intermediate artifacts *inside it*.\n",
    "\n",
    "### File selection logic\n",
    "\n",
    "- **`INCLUDE_EXTENSIONS`** controls which file types are copied.\n",
    "- Slurm stdout/stderr files are also included via a name pattern (default: `slurm*`).\n",
    "\n",
    "### Observation/data-dictionary logic\n",
    "\n",
    "- The data dictionary step targets one class of CSV outputs.\n",
    "- By default, it searches for files matching `water_balance*.csv`.\n",
    "\n",
    "If your simulation uses different names, change `OBS_FILE_HANDLE` and/or `OBS_FILE_FORMAT`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441ed507",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Config:\n",
    "    # Root working location (HPC scratch or local path)\n",
    "    scratch_root: Path\n",
    "\n",
    "    # Source (simulation) and destination (curated data package)\n",
    "    simulation_dir: Path\n",
    "    data_pkg_dir: Path\n",
    "\n",
    "    # What to copy\n",
    "    include_extensions: Tuple[str, ...]\n",
    "    include_name_globs: Tuple[str, ...]\n",
    "\n",
    "    # Optional post-copy cleanup patterns\n",
    "    #\n",
    "    # Many ATS simulations write outputs under directories whose names contain\n",
    "    # tokens like run0, run1, run2, etc. Not every user has every run token.\n",
    "    # This notebook therefore treats these tokens as *optional*:\n",
    "    #   - If at least one matching run directory is found, cleanup will run\n",
    "    #     on the directories that exist.\n",
    "    #   - If none are found, the cleanup cell raises an error (because it has\n",
    "    #     nothing to operate on).\n",
    "    run_tokens: Tuple[str, ...]\n",
    "    keep_checkpoint_token: str\n",
    "\n",
    "    # Data dictionary targeting\n",
    "    obs_file_handle: str\n",
    "    obs_file_format: str\n",
    "\n",
    "    # Header parsing\n",
    "    header_line_hint: Optional[int] = None\n",
    "    header_probe_max_lines: int = 400\n",
    "\n",
    "    # If you want to write cleaned CSVs (avoid in-place edits by default)\n",
    "    write_new_csv: bool = False\n",
    "    inplace: bool = False  # Only used if write_new_csv is True\n",
    "\n",
    "\n",
    "SCRATCH_ROOT = Path(os.environ.get(\"SCRATCH\") or Path.cwd())\n",
    "\n",
    "cfg = Config(\n",
    "    scratch_root=SCRATCH_ROOT,\n",
    "    simulation_dir=SCRATCH_ROOT / \"Naches\",  # EDIT ME\n",
    "    data_pkg_dir=SCRATCH_ROOT / \"my_ATS_MDA\",  # EDIT ME\n",
    "    include_extensions=(\n",
    "        \"exo\", \"xml\", \"csv\", \"dat\", \"txt\", \"xmf\", \"h5\", \"out\", \"nc\", \"jpg\", \"png\", \"pdf\"\n",
    "    ),\n",
    "    include_name_globs=(\"slurm*\",),\n",
    "    run_tokens=(\"run0\", \"run1\", \"run2\"),\n",
    "    keep_checkpoint_token=\"final\",  # keep checkpoint*final*.h5\n",
    "    obs_file_handle=\"water_balance\",\n",
    "    obs_file_format=\"csv\",\n",
    "    # If auto-detection fails, set a manual line index (0-based).\n",
    "    # header_line_hint=112,\n",
    "    write_new_csv=False,\n",
    "    inplace=False,\n",
    ")\n",
    "\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63296b68",
   "metadata": {},
   "source": [
    "## 1) Validate paths and prepare destination\n",
    "\n",
    "Before copying anything, confirm:\n",
    "\n",
    "- `cfg.simulation_dir` points to the directory that contains your finished ATS run outputs.\n",
    "- `cfg.data_pkg_dir` is a **new or disposable** directory where the notebook can place a curated copy.\n",
    "\n",
    "The notebook will:\n",
    "\n",
    "- create `cfg.data_pkg_dir` if needed;\n",
    "- log the source/destination sizes so you can sanity-check that the curated copy is smaller than the full run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586eea98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate source directory\n",
    "if not cfg.simulation_dir.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Simulation directory does not exist: {cfg.simulation_dir}\\n\"\n",
    "        \"Update cfg.simulation_dir in the Configuration section.\"\n",
    "    )\n",
    "logger.info(\"Simulation directory: %s\", cfg.simulation_dir)\n",
    "\n",
    "# Prepare destination\n",
    "ensure_dir(cfg.data_pkg_dir)\n",
    "logger.info(\"Data package directory: %s\", cfg.data_pkg_dir)\n",
    "\n",
    "# Optional: report current size (destination might already have content)\n",
    "try:\n",
    "    src_size = dir_size_bytes(cfg.simulation_dir)\n",
    "    dst_size = dir_size_bytes(cfg.data_pkg_dir)\n",
    "    logger.info(\"Source size: %s\", human_bytes(src_size))\n",
    "    logger.info(\"Destination current size: %s\", human_bytes(dst_size))\n",
    "except Exception as e:\n",
    "    logger.warning(\"Could not compute directory sizes: %s\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8563b4f0",
   "metadata": {},
   "source": [
    "## 2) Copy publishable artifacts into the data package directory\n",
    "\n",
    "This step creates the curated package by copying only selected files.\n",
    "\n",
    "### Why use `rsync` include/exclude rules?\n",
    "\n",
    "- It preserves directory structure.\n",
    "- It is fast for large directories.\n",
    "- It avoids copying unwanted file types up front.\n",
    "\n",
    "The include rules are:\n",
    "\n",
    "- Include all folders (`--include=*/`) so the traversal works.\n",
    "- Include file extensions listed in `cfg.include_extensions`.\n",
    "- Include any filename globs listed in `cfg.include_name_globs` (e.g., `slurm*`).\n",
    "- Exclude everything else.\n",
    "\n",
    "If `rsync` is not available, the notebook falls back to a pure-Python copy that preserves relative paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7639a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build include arguments\n",
    "include_args: List[str] = []\n",
    "for ext in cfg.include_extensions:\n",
    "    include_args.append(f\"--include=*.{ext}\")\n",
    "for glob_pat in cfg.include_name_globs:\n",
    "    include_args.append(f\"--include={glob_pat}\")\n",
    "\n",
    "rsync_path = which(\"rsync\")\n",
    "\n",
    "if rsync_path:\n",
    "    # NOTE: `-P` enables `--partial --progress` (per-file progress).\n",
    "    # We intentionally run rsync in \"live\" mode so the progress is visible.\n",
    "    cmd = [\n",
    "        \"rsync\",\n",
    "        \"-avhP\",\n",
    "        \"--include=*/\",\n",
    "        *include_args,\n",
    "        \"--exclude=*\",\n",
    "        \"--prune-empty-dirs\",\n",
    "        f\"{str(cfg.simulation_dir).rstrip('/')}/\",\n",
    "        str(cfg.data_pkg_dir),\n",
    "    ]\n",
    "\n",
    "    run_cmd_live(cmd)\n",
    "else:\n",
    "    logger.warning(\"rsync not found; using a slower Python-based copy.\")\n",
    "\n",
    "    allowed_exts = {f\".{e.lower()}\" for e in cfg.include_extensions}\n",
    "\n",
    "    for src in cfg.simulation_dir.rglob(\"*\"):\n",
    "        if not src.is_file():\n",
    "            continue\n",
    "\n",
    "        if src.suffix.lower() in allowed_exts or any(src.match(p) for p in cfg.include_name_globs):\n",
    "            rel = src.relative_to(cfg.simulation_dir)\n",
    "            dst = cfg.data_pkg_dir / rel\n",
    "            ensure_dir(dst.parent)\n",
    "            shutil.copy2(src, dst)\n",
    "\n",
    "logger.info(\"Copy complete. Destination size: %s\", human_bytes(dir_size_bytes(cfg.data_pkg_dir)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3992331",
   "metadata": {},
   "source": [
    "## 3) Post-copy cleanup (remove bulky intermediates)\n",
    "\n",
    "Publication packages should generally exclude:\n",
    "\n",
    "- **Periodic checkpoints** that are not required to reproduce or understand final results.\n",
    "- **Visualization time-series files** that can be regenerated and are very large.\n",
    "- **Paraview `.xmf`** metadata files (optional; include them only if they are part of your reproducibility story).\n",
    "\n",
    "### Run directories are not guaranteed\n",
    "\n",
    "Different users and workflows may have different run directory layouts. A common convention is to place outputs under\n",
    "folders whose names include tokens like `run0`, `run1`, or `run2`, but it is **not required** that all (or any specific)\n",
    "run tokens exist.\n",
    "\n",
    "This notebook therefore implements the following rule:\n",
    "\n",
    "- If the package contains **at least one** directory whose name includes any of the tokens in `cfg.run_tokens`\n",
    "  (default: `run0`, `run1`, `run2`), cleanup will operate on **only the directories that exist**.\n",
    "- If the package contains **none** of those directories, the cleanup step raises an error.\n",
    "\n",
    "### Conservative default cleanup rules\n",
    "\n",
    "- Remove `checkpoint*.h5` that do **not** contain the token configured as `cfg.keep_checkpoint_token` (default: `final`).\n",
    "- Remove all `*.xmf` under detected run directories.\n",
    "- Remove all `ats_vis_*.h5` under detected run directories.\n",
    "\n",
    "Adjust these rules to match your publication and reproducibility requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0cb83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_run_dirs(tokens: Iterable[str], root: Path) -> Dict[str, Path]:\n",
    "    \"\"\"Find run directories whose names contain any of the provided tokens.\n",
    "\n",
    "    Requirements\n",
    "    ------------\n",
    "    - Users may have *some* of run0/run1/run2 (or none).\n",
    "    - If none are found, raise an error.\n",
    "    - If one or more are found, return only the ones that exist.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    We match directories by substring using `*{token}*` so layouts like\n",
    "    `spinup_run1` or `caseA/run0_foo` are detected.\n",
    "    \"\"\"\n",
    "    found: Dict[str, Path] = {}\n",
    "\n",
    "    tokens_tuple = tuple(tokens)\n",
    "    for token in tokens_tuple:\n",
    "        candidates = [p for p in root.rglob(f\"*{token}*\") if p.is_dir()]\n",
    "        if not candidates:\n",
    "            continue\n",
    "\n",
    "        # Prefer the shallowest path (fewest parts) to avoid nested hits.\n",
    "        candidates = sorted(candidates, key=lambda p: (len(p.parts), str(p)))\n",
    "        chosen = candidates[0]\n",
    "        found[token] = chosen\n",
    "\n",
    "        if len(candidates) > 1:\n",
    "            logger.warning(\n",
    "                \"Multiple candidates found for %s; choosing %s (alternatives: %s)\",\n",
    "                token,\n",
    "                chosen,\n",
    "                \", \".join(str(c) for c in candidates[1:5]),\n",
    "            )\n",
    "\n",
    "    if not found:\n",
    "        raise FileNotFoundError(\n",
    "            f\"Could not find any run directories matching {tokens_tuple} under {root}.\\n\"\n",
    "            \"Expected at least one directory containing one of: run0, run1, run2 (or the tokens you configured).\"\n",
    "        )\n",
    "\n",
    "    return found\n",
    "\n",
    "\n",
    "run_dirs = find_run_dirs(cfg.run_tokens, cfg.data_pkg_dir)\n",
    "run_dirs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541628c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-final checkpoints, xmf, and visualization files\n",
    "removed = {\n",
    "    \"non_final_checkpoints\": 0,\n",
    "    \"xmf\": 0,\n",
    "    \"ats_vis\": 0,\n",
    "}\n",
    "\n",
    "for token, run_dir in run_dirs.items():\n",
    "    # 1) Checkpoints\n",
    "    checkpoints = list(run_dir.rglob(\"checkpoint*.h5\"))\n",
    "    non_final = [p for p in checkpoints if cfg.keep_checkpoint_token not in p.name]\n",
    "    for p in non_final:\n",
    "        p.unlink(missing_ok=True)\n",
    "        removed[\"non_final_checkpoints\"] += 1\n",
    "\n",
    "    # 2) XMF files\n",
    "    for p in run_dir.rglob(\"*.xmf\"):\n",
    "        p.unlink(missing_ok=True)\n",
    "        removed[\"xmf\"] += 1\n",
    "\n",
    "    # 3) Visualization time-series\n",
    "    for p in run_dir.rglob(\"ats_vis_*.h5\"):\n",
    "        p.unlink(missing_ok=True)\n",
    "        removed[\"ats_vis\"] += 1\n",
    "\n",
    "logger.info(\"Cleanup summary: %s\", removed)\n",
    "logger.info(\"Destination size after cleanup: %s\", human_bytes(dir_size_bytes(cfg.data_pkg_dir)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0576fc02",
   "metadata": {},
   "source": [
    "## 4) Enumerate all files and generate `flmd.csv`\n",
    "\n",
    "The *file-level metadata* table is typically required by repositories to:\n",
    "\n",
    "- provide a complete inventory of files;\n",
    "- explain what each file is and how it relates to the dataset;\n",
    "- support discovery and reuse.\n",
    "\n",
    "This notebook generates a first-pass `flmd.csv` that you can edit.\n",
    "\n",
    "### Notes on the fields\n",
    "\n",
    "- `File_Name`: basename only\n",
    "- `File_Path`: relative folder path (with trailing slash)\n",
    "- `File_Description`: intentionally left for you to refine, but we add a few auto-filled hints based on filename patterns\n",
    "- `Start_Date` / `End_Date`: left as `-9999` placeholders (replace if you have file-level temporal coverage)\n",
    "\n",
    "After creation, open the CSV in a spreadsheet editor and improve descriptions as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5101e205",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = list_files(cfg.data_pkg_dir)\n",
    "logger.info(\"Total files in package: %d\", len(all_files))\n",
    "\n",
    "file_names, file_paths = rel_manifest_rows(cfg.data_pkg_dir, all_files)\n",
    "\n",
    "flmd = pd.DataFrame(\n",
    "    {\n",
    "        \"File_Name\": file_names,\n",
    "        \"File_Description\": [\"\" for _ in all_files],\n",
    "        \"Standard\": [\"N/A\" for _ in all_files],\n",
    "        \"Header_Rows\": [-9999 for _ in all_files],\n",
    "        \"Column_or_Row_Name_Position\": [-9999 for _ in all_files],\n",
    "        \"File_Path\": file_paths,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Auto-fill some common patterns to reduce manual effort\n",
    "pattern_to_desc = {\n",
    "    \"water_balance\": \"Model output ASCII/CSV file (water balance diagnostics).\",\n",
    "    \"checkpoint\": \"ATS checkpoint file (binary restart/state).\",\n",
    "    \"ats_vis\": \"ATS visualization output file (binary; often large).\",\n",
    "    \"pflotran\": \"Output file from PFLOTRAN (biogeochemistry engine).\",\n",
    "    \"slurm\": \"Scheduler (Slurm) stdout/stderr log for the run.\",\n",
    "    \"xml\": \"ATS input/configuration file.\",\n",
    "    \"exo\": \"Computational mesh file.\",\n",
    "    \"LAI\": \"Leaf Area Index input forcing.\",\n",
    "}\n",
    "\n",
    "for pat, desc in pattern_to_desc.items():\n",
    "    mask = flmd[\"File_Name\"].str.contains(pat, regex=False)\n",
    "    flmd.loc[mask, \"File_Description\"] = desc\n",
    "\n",
    "flmd_path = cfg.data_pkg_dir / \"flmd.csv\"\n",
    "flmd.to_csv(flmd_path, index=False)\n",
    "logger.info(\"Wrote: %s\", flmd_path)\n",
    "\n",
    "flmd.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffd841f",
   "metadata": {},
   "source": [
    "## 5) Build a data dictionary (`dd.csv`) from a representative output CSV\n",
    "\n",
    "This section focuses on tabular data intended for reuse (e.g., time series diagnostics).\n",
    "\n",
    "### How the parsing works\n",
    "\n",
    "1. Find files matching `f\"{cfg.obs_file_handle}*.{cfg.obs_file_format}\"`.\n",
    "2. Read a small number of initial lines.\n",
    "3. Detect the header line (or use `cfg.header_line_hint` if set).\n",
    "4. Parse header tokens of the form `name [unit]`.\n",
    "\n",
    "The result is a `dd.csv` with:\n",
    "\n",
    "- `Column_or_Row_Name`\n",
    "- `Unit`\n",
    "- `Definition` (auto-filled later when possible)\n",
    "- `Data_Type`\n",
    "- `Term_Type`\n",
    "\n",
    "If your CSV format differs (e.g., units in a second header row), adjust the parsing functions in the setup cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94835ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate candidate CSVs\n",
    "csv_files = sorted(cfg.data_pkg_dir.rglob(f\"{cfg.obs_file_handle}*.{cfg.obs_file_format}\"))\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No files found matching {cfg.obs_file_handle}*.{cfg.obs_file_format} under {cfg.data_pkg_dir}\"\n",
    "    )\n",
    "\n",
    "logger.info(\"Found %d candidate CSV files. Using the first as a schema reference: %s\", len(csv_files), csv_files[0])\n",
    "\n",
    "ref_csv = csv_files[0]\n",
    "lines = ref_csv.read_text(errors=\"replace\").splitlines(keepends=True)\n",
    "\n",
    "if cfg.header_line_hint is not None:\n",
    "    header_idx = cfg.header_line_hint\n",
    "else:\n",
    "    header_idx = detect_header_line(lines, max_lines=cfg.header_probe_max_lines)\n",
    "\n",
    "logger.info(\"Using header line index: %d\", header_idx)\n",
    "header_line = lines[header_idx]\n",
    "\n",
    "parameters, units = parse_ats_csv_header_line(header_line)\n",
    "logger.info(\"Parsed %d parameters\", len(parameters))\n",
    "\n",
    "# Optional rewriting of CSV headers (disabled by default)\n",
    "if cfg.write_new_csv:\n",
    "    for p in csv_files:\n",
    "        content = p.read_text(errors=\"replace\").splitlines(keepends=True)\n",
    "        content[header_idx] = \",\".join([f\"{a} [{b}]\" if b != \"N/A\" else a for a, b in zip(parameters, units)]) + \"\\n\"\n",
    "\n",
    "        out_path = p if cfg.inplace else p.with_name(p.name + \"_tmp\")\n",
    "        out_path.write_text(\"\".join(content))\n",
    "\n",
    "# Create dd.csv skeleton\n",
    "\n",
    "dd = pd.DataFrame(\n",
    "    {\n",
    "        \"Column_or_Row_Name\": parameters,\n",
    "        \"Unit\": units,\n",
    "        \"Definition\": [\"\" for _ in parameters],\n",
    "        \"Data_Type\": [\"numeric\" for _ in parameters],\n",
    "        \"Term_Type\": [\"column_header\" for _ in parameters],\n",
    "        \"Missing_Value_Code\": ['\"N/A\"; \"-9999\"; \"\"; \"NA\"' for _ in parameters],\n",
    "        \"Reported_Precision\": [1E-16 for _ in parameters],\n",
    "    }\n",
    ")\n",
    "\n",
    "dd.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f695c271",
   "metadata": {},
   "source": [
    "## 6) Auto-fill definitions from ATS input-spec documentation (best effort)\n",
    "\n",
    "ATS publishes an input-specification *symbol table* that maps many variable root names to descriptions.\n",
    "\n",
    "This notebook attempts to:\n",
    "\n",
    "1. Download the symbol table (an org-mode table) from the ATS GitHub repository.\n",
    "2. Parse it into a lookup map.\n",
    "3. Fill `dd[\"Definition\"]` by matching the *longest* symbol-table key that appears as a substring of each column name.\n",
    "\n",
    "Important limitations:\n",
    "\n",
    "- Not every output column will have a corresponding symbol-table entry.\n",
    "- Matching is heuristic; you must review the definitions.\n",
    "- Network access is required for the download step.\n",
    "\n",
    "If you are working offline, skip the download and fill definitions manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9015b9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and parse symbol table\n",
    "symbol_table_url = (\n",
    "    \"https://raw.githubusercontent.com/amanzi/ats/refs/heads/master/\"\n",
    "    \"docs/documentation/source/input_spec/symbol_table.org\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    resp = requests.get(symbol_table_url, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    df_symbol = parse_org_table(resp.text)\n",
    "    definition_map = build_definition_map_from_symbol_table(df_symbol)\n",
    "\n",
    "    logger.info(\"Loaded %d symbol-table entries\", len(definition_map))\n",
    "\n",
    "    dd[\"Definition\"] = fill_definitions(dd[\"Column_or_Row_Name\"].tolist(), definition_map)\n",
    "\n",
    "    missing = (dd[\"Definition\"].str.strip() == \"\").sum()\n",
    "    logger.info(\"Columns without auto-filled definitions: %d / %d\", missing, len(dd))\n",
    "\n",
    "except Exception as e:\n",
    "    logger.warning(\"Could not download/parse symbol table: %s\", e)\n",
    "    logger.warning(\"Proceeding with empty definitions; fill dd.csv manually.\")\n",
    "\n",
    "# Write dd.csv\n",
    "\n",
    "dd_path = cfg.data_pkg_dir / \"dd.csv\"\n",
    "dd.to_csv(dd_path, index=False)\n",
    "logger.info(\"Wrote: %s\", dd_path)\n",
    "\n",
    "dd.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496b59a9",
   "metadata": {},
   "source": [
    "## 7) Optional: package integrity checks (recommended)\n",
    "\n",
    "Before submission, it is good practice to:\n",
    "\n",
    "- spot-check the curated directory content;\n",
    "- record the total size;\n",
    "- (optionally) generate a checksum manifest.\n",
    "\n",
    "The cell below generates a simple `sha256sums.txt` file for all files in the package.\n",
    "This is optional, but can help you verify upload integrity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f4c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "checksum_path = cfg.data_pkg_dir / \"sha256sums.txt\"\n",
    "\n",
    "with checksum_path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for p in list_files(cfg.data_pkg_dir):\n",
    "        # Skip checksum file itself to keep the manifest stable\n",
    "        if p.name == checksum_path.name:\n",
    "            continue\n",
    "\n",
    "        h = hashlib.sha256()\n",
    "        with p.open(\"rb\") as fp:\n",
    "            for chunk in iter(lambda: fp.read(1024 * 1024), b\"\"):\n",
    "                h.update(chunk)\n",
    "\n",
    "        rel = p.relative_to(cfg.data_pkg_dir).as_posix()\n",
    "        f.write(f\"{h.hexdigest()}  {rel}\\n\")\n",
    "\n",
    "logger.info(\"Wrote checksums: %s\", checksum_path)\n",
    "logger.info(\"Final package size: %s\", human_bytes(dir_size_bytes(cfg.data_pkg_dir)))\n",
    "\n",
    "checksum_path.read_text().splitlines()[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f114059e",
   "metadata": {},
   "source": [
    "## 8) Suggested next step: upload this ATS MDA package to ESS-DIVE\n",
    "\n",
    "At this point, your curated ATS MDA directory (`cfg.data_pkg_dir`) should contain:\n",
    "\n",
    "- The data files you intend to publish (e.g., model outputs, inputs needed for reproducibility)\n",
    "- `flmd.csv` (File Level Metadata)\n",
    "- `dd.csv` (Data Dictionary)\n",
    "- (Optional) `checksums.sha256` if you ran the integrity-check cell\n",
    "\n",
    "### Option A: Submit with the ESS-DIVE online form (recommended for small/medium datasets)\n",
    "\n",
    "1. Create (or sign in to) your ESS-DIVE contributor account.\n",
    "2. Start a new dataset submission using the **online data submission form**.\n",
    "3. Upload the files in `cfg.data_pkg_dir`.\n",
    "4. Fill in dataset-level metadata (title, abstract, authors, funding award numbers, temporal/spatial coverage, etc.).\n",
    "5. Initiate the review/publish cycle to request a DOI.\n",
    "\n",
    "Helpful ESS-DIVE docs:\n",
    "- \"Get Started\" (contributing data)\n",
    "- \"Submit Data with Online Form\"\n",
    "- \"Publish your Dataset\"\n",
    "\n",
    "### Option B: Programmatic submission with the ESS-DIVE Dataset API (recommended for large, repeated, or automated submissions)\n",
    "\n",
    "If you plan recurring uploads (e.g., many parameter sweeps) or want automated pipelines:\n",
    "\n",
    "1. Ensure your dataset metadata meets ESS-DIVE requirements.\n",
    "2. Prepare a JSON-LD metadata document (per ESS-DIVE documentation).\n",
    "3. Use the Dataset API to upload files from a local directory and submit metadata.\n",
    "\n",
    "Helpful ESS-DIVE docs:\n",
    "- \"Submit Data with the Dataset API\"\n",
    "- \"ESS-DIVE Dataset API\" overview\n",
    "\n",
    "### Practical recommendations before you upload\n",
    "\n",
    "- Keep `flmd.csv` and `dd.csv` alongside your data files so ESS-DIVE tooling can parse and index your dataset.\n",
    "- If you publish multiple observation files, consider generating a data dictionary per file or using wildcards in dd fields (per ESS-DIVE guidance).\n",
    "- For very large uploads (multi-GB to TB scale), contact ESS-DIVE support to choose the best transfer mechanism.\n",
    "\n",
    "### Optional: create a single archive for upload\n",
    "\n",
    "Some users prefer to upload a single archive (and/or stage it somewhere for transfer):\n",
    "\n",
    "```bash\n",
    "cd \"${DATA_PKG_DIR}\"\n",
    "tar -czf ats_mda_package.tar.gz .\n",
    "```\n",
    "\n",
    "Replace `${DATA_PKG_DIR}` with your `cfg.data_pkg_dir` path.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f590f5e",
   "metadata": {},
   "source": [
    "## 9) Create per-subdirectory tarballs for transfer / upload (with automatic splitting > 5 GB)\n",
    "\n",
    "Large ATS MDA directories can be difficult to upload as a single payload, especially when working over slow links.\n",
    "A common practice is to archive each *top-level subdirectory* (e.g., `run0/`, `run1/`, `run2/`) into its own compressed tarball:\n",
    "\n",
    "- `run0.tar.gz`\n",
    "- `run1.tar.gz`\n",
    "- `run2.tar.gz`\n",
    "\n",
    "This section:\n",
    "\n",
    "1. Creates `*.tar.gz` archives **for each immediate subdirectory** under `cfg.data_pkg_dir`.\n",
    "2. Reports the size of each archive.\n",
    "3. If an archive exceeds **5 GiB**, splits it into `5G` parts using `split`, producing files like:\n",
    "   - `run0.tar.gz.part001`, `run0.tar.gz.part002`, ...\n",
    "\n",
    "You can upload either:\n",
    "\n",
    "- the intact `*.tar.gz` files (when they are small enough), or\n",
    "- the `*.partNNN` files (for large archives), and reconstruct them later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ce9acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tarball + optional split utilities ---\n",
    "\n",
    "def make_tar_gz_per_subdir(\n",
    "    mda_dir: Path,\n",
    "    *,\n",
    "    output_dir: Optional[Path] = None,\n",
    "    max_part_gib: int = 5,\n",
    "    overwrite: bool = False,\n",
    "    split_large_archives: bool = True,\n",
    "    remove_original_after_split: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Create one .tar.gz per immediate subdirectory of ``mda_dir``.\n",
    "\n",
    "    This is useful when you want to transfer or upload a large ATS MDA package in\n",
    "    smaller, logically grouped pieces (e.g., one archive per ``runX/`` directory).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mda_dir\n",
    "        The ATS MDA directory (e.g., ``cfg.data_pkg_dir``).\n",
    "    output_dir\n",
    "        Where to write archives. Defaults to ``mda_dir``.\n",
    "    max_part_gib\n",
    "        Split threshold and chunk size, in GiB. Default: 5.\n",
    "    overwrite\n",
    "        If True, overwrite existing ``*.tar.gz`` archives *and* any existing split parts.\n",
    "    split_large_archives\n",
    "        If True, archives larger than ``max_part_gib`` will be split with ``split -b {max_part_gib}G``.\n",
    "    remove_original_after_split\n",
    "        If True, delete the original ``*.tar.gz`` after splitting (saves space, but destructive).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Summary table with sizes and split outputs.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This function archives *each immediate subdirectory* under ``mda_dir``.\n",
    "      Files located directly in ``mda_dir`` (e.g., ``flmd.csv`` and ``dd.csv``)\n",
    "      are **not** included in these per-subdirectory archives.\n",
    "    - The splitting step prefers GNU ``split`` flags to create ``.part001`` style\n",
    "      suffixes. If those flags are not supported, it falls back to POSIX ``split``\n",
    "      and renames outputs to ``.part001``, ``.part002``, ...\n",
    "    \"\"\"\n",
    "\n",
    "    def _remove_existing_parts(out_dir: Path, tar_name: str) -> None:\n",
    "        for p in out_dir.glob(tar_name + \".part*\"):\n",
    "            try:\n",
    "                p.unlink()\n",
    "            except Exception as e:\n",
    "                logger.warning(\"Could not remove existing part %s: %s\", p, e)\n",
    "\n",
    "    def _split_archive(tar_path: Path, out_dir: Path) -> List[Path]:\n",
    "        \"\"\"Split ``tar_path`` into ``max_part_gib`` chunks, returning the part file paths.\"\"\"\n",
    "        prefix = str(tar_path) + \".part\"  # desired prefix -> <tar>.part001\n",
    "\n",
    "        # If parts already exist and we're not overwriting, reuse them.\n",
    "        existing = sorted(out_dir.glob(tar_path.name + \".part*\"))\n",
    "        if existing and not overwrite:\n",
    "            logger.info(\"Split parts already exist (reusing): %s (%d parts)\", tar_path.name, len(existing))\n",
    "            return existing\n",
    "\n",
    "        # If overwriting, clean up old parts first.\n",
    "        if existing and overwrite:\n",
    "            _remove_existing_parts(out_dir, tar_path.name)\n",
    "\n",
    "        # Preferred GNU split invocation.\n",
    "        gnu_cmd = [\n",
    "            split_exe,\n",
    "            \"-b\",\n",
    "            f\"{max_part_gib}G\",\n",
    "            \"--numeric-suffixes=1\",\n",
    "            \"--suffix-length=3\",\n",
    "            str(tar_path),\n",
    "            prefix,\n",
    "        ]\n",
    "\n",
    "        try:\n",
    "            run_cmd(gnu_cmd)\n",
    "            parts = sorted(out_dir.glob(tar_path.name + \".part*\"))\n",
    "            if parts:\n",
    "                return parts\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            logger.warning(\n",
    "                \"GNU-style split flags not supported on this system; falling back to POSIX split. Error: %s\",\n",
    "                str(e),\n",
    "            )\n",
    "\n",
    "        # POSIX fallback: split -b 5G <file> <prefix>\n",
    "        posix_cmd = [split_exe, \"-b\", f\"{max_part_gib}G\", str(tar_path), prefix]\n",
    "        run_cmd(posix_cmd)\n",
    "\n",
    "        # POSIX split produces suffixes like aa, ab, ac... Rename to numeric partNNN.\n",
    "        raw_parts = sorted(out_dir.glob(tar_path.name + \".part*\"))\n",
    "        if not raw_parts:\n",
    "            raise RuntimeError(f\"Split reported success but no part files found for: {tar_path}\")\n",
    "\n",
    "        # Rename deterministically in lexical order.\n",
    "        renamed = []\n",
    "        for i, p in enumerate(raw_parts, start=1):\n",
    "            new = p.with_name(f\"{tar_path.name}.part{i:03d}\")\n",
    "            if new.exists() and not overwrite:\n",
    "                raise FileExistsError(\n",
    "                    f\"Refusing to overwrite existing part file: {new}. Set overwrite=True to replace.\"\n",
    "                )\n",
    "            p.rename(new)\n",
    "            renamed.append(new)\n",
    "\n",
    "        return renamed\n",
    "\n",
    "    mda_dir = Path(mda_dir)\n",
    "    if output_dir is None:\n",
    "        output_dir = mda_dir\n",
    "    output_dir = Path(output_dir)\n",
    "\n",
    "    if not mda_dir.exists():\n",
    "        raise FileNotFoundError(f\"MDA directory does not exist: {mda_dir}\")\n",
    "\n",
    "    ensure_dir(output_dir)\n",
    "\n",
    "    tar_exe = which(\"tar\")\n",
    "    if not tar_exe:\n",
    "        raise RuntimeError(\"Required executable 'tar' not found on PATH.\")\n",
    "\n",
    "    split_exe = which(\"split\")\n",
    "    if split_large_archives and not split_exe:\n",
    "        raise RuntimeError(\"Requested splitting, but required executable 'split' not found on PATH.\")\n",
    "\n",
    "    max_bytes = int(max_part_gib) * 1024**3\n",
    "\n",
    "    subdirs = sorted([p for p in mda_dir.iterdir() if p.is_dir() and not p.name.startswith(\".\")])\n",
    "    if not subdirs:\n",
    "        raise FileNotFoundError(f\"No subdirectories found under {mda_dir} (nothing to archive).\")\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for sd in subdirs:\n",
    "        tar_path = output_dir / f\"{sd.name}.tar.gz\"\n",
    "\n",
    "        # Optionally clean up existing tar + parts if overwriting.\n",
    "        if overwrite:\n",
    "            if tar_path.exists():\n",
    "                try:\n",
    "                    tar_path.unlink()\n",
    "                except Exception as e:\n",
    "                    raise RuntimeError(f\"Could not remove existing archive {tar_path}: {e}\")\n",
    "            _remove_existing_parts(output_dir, tar_path.name)\n",
    "\n",
    "        if tar_path.exists() and not overwrite:\n",
    "            logger.info(\"Archive exists (skipping creation): %s\", tar_path)\n",
    "        else:\n",
    "            # Create archive with a stable relative layout: <subdir>/...\n",
    "            # -C mda_dir ensures tar stores paths relative to mda_dir.\n",
    "            cmd = [tar_exe, \"-czf\", str(tar_path), \"-C\", str(mda_dir), sd.name]\n",
    "            logger.info(\"Creating archive: %s\", tar_path)\n",
    "            run_cmd(cmd)\n",
    "\n",
    "        if not tar_path.exists():\n",
    "            raise FileNotFoundError(f\"Archive was not created: {tar_path}\")\n",
    "\n",
    "        size_bytes = tar_path.stat().st_size\n",
    "        did_split = False\n",
    "        part_files: List[Path] = []\n",
    "\n",
    "        if split_large_archives and size_bytes > max_bytes:\n",
    "            did_split = True\n",
    "            logger.info(\n",
    "                \"Archive > %d GiB (%s). Splitting into %d GiB parts: %s\",\n",
    "                max_part_gib,\n",
    "                human_bytes(size_bytes),\n",
    "                max_part_gib,\n",
    "                tar_path.name,\n",
    "            )\n",
    "\n",
    "            part_files = _split_archive(tar_path, output_dir)\n",
    "\n",
    "            if remove_original_after_split:\n",
    "                logger.info(\"Removing original archive after split: %s\", tar_path)\n",
    "                tar_path.unlink()\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"subdir\": sd.name,\n",
    "                \"archive\": tar_path.name,\n",
    "                \"archive_path\": str(tar_path),\n",
    "                \"archive_size_bytes\": size_bytes,\n",
    "                \"archive_size\": human_bytes(size_bytes),\n",
    "                \"split\": did_split,\n",
    "                \"num_parts\": len(part_files),\n",
    "                \"parts\": \", \".join([p.name for p in part_files]) if part_files else \"\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values([\"split\", \"archive_size_bytes\"], ascending=[True, False])\n",
    "    return df\n",
    "\n",
    "\n",
    "# --- Run packaging on your curated MDA directory ---\n",
    "\n",
    "df_archives = make_tar_gz_per_subdir(\n",
    "    cfg.data_pkg_dir,\n",
    "    output_dir=cfg.data_pkg_dir,\n",
    "    max_part_gib=5,\n",
    "    overwrite=False,\n",
    "    split_large_archives=True,\n",
    "    remove_original_after_split=False,\n",
    ")\n",
    "\n",
    "# Report sizes clearly\n",
    "pd.set_option(\"display.max_colwidth\", 140)\n",
    "logger.info(\n",
    "    \"Archive summary\\n%s\",\n",
    "    df_archives[[\"subdir\", \"archive\", \"archive_size\", \"split\", \"num_parts\"]].to_string(index=False),\n",
    ")\n",
    "\n",
    "# Display as a table in the notebook\n",
    "df_archives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56b72fe",
   "metadata": {},
   "source": [
    "### Reconstructing a split archive (`*.partNNN` → `*.tar.gz`)\n",
    "\n",
    "If a tarball was split (e.g., `run0.tar.gz.part001`, `run0.tar.gz.part002`, ...), you can reassemble it on a Linux/HPC system as follows:\n",
    "\n",
    "1. **Ensure all parts are present in the same directory**.\n",
    "2. Reconstruct the tarball (the numeric suffix padding ensures correct ordering):\n",
    "\n",
    "```bash\n",
    "cat run0.tar.gz.part* > run0.tar.gz\n",
    "```\n",
    "\n",
    "3. (Optional) Verify the reconstructed archive:\n",
    "\n",
    "```bash\n",
    "gzip -t run0.tar.gz\n",
    "# or list a few entries\n",
    "tar -tzf run0.tar.gz | head\n",
    "```\n",
    "\n",
    "4. Extract it:\n",
    "\n",
    "```bash\n",
    "tar -xzf run0.tar.gz\n",
    "```\n",
    "\n",
    "Notes:\n",
    "\n",
    "- If you are transferring over a network, consider also transferring the `sha256sums.txt` manifest (Section 7) and verifying checksums after reconstruction.\n",
    "- If you set `remove_original_after_split=True`, only the parts will remain (saving scratch space). If you keep the original tarball, you can upload either the whole tarball or its parts—do not upload both unless you intend to.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
