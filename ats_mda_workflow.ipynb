{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8db0540-dcec-4464-bddf-3afa29acb6c1",
   "metadata": {},
   "source": [
    "# ATS MDA workflow\n",
    "\n",
    "This workflow is a step-by-step guide to archive ATS model data associated with a manuscript on ESS-DIVE (https://docs.ess-dive.lbl.gov/). Purpose of this guide is to standardize and automate the data archving process. \n",
    "\n",
    "A few key concepts in the notebook:\n",
    "\n",
    "- `Simulation Directory`: where ATS runs (e.g., a directory at $SCRATCH on HPC)\n",
    "- `Data Package Directory`: where you want to create the data archive (it can be anywhere on HPC or local machine)\n",
    "- `rsync`: a file transfer command line tool (use `apt install rsync` or `brew install rsync` to install it on Linux or macOS, respectively)\n",
    "\n",
    "|     | where to archive | extention examples |\n",
    "| --- | --- | --- |\n",
    "| Files related to the manuscript | ESS-DIVE | exo, xml, h5, nc, csv |\n",
    "| Files not related to the manuscript | HPSS | all |\n",
    "\n",
    "This command below is to sync all files with certain extentions in the simulation directory to the data package directory.\n",
    "\n",
    "```\n",
    "rsync -avzP --include=*/ --include='*.exo' --include='*.xml' --include='*.h5' --include='*.nc' --include='*.csv' --exclude=* --prune-empty-dirs ./<Simulation Directory>/ ./<Data Package Directory>/\n",
    "```\n",
    "\n",
    "This workflow creates the following files that are needed in the data archiving step:\n",
    "\n",
    "* flmd.csv : File Level Metadata\n",
    "> The file level metadata is a csv containing a list of all files in the data package and descriptions of what the files contain.\n",
    "\n",
    "* dd.csv : Data Dictionary\n",
    "> The data dictionary is a csv file that lists all column and row headers in the data package's tabular files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f53e8d4-557d-43d9-8887-4d0c2b44430f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import subprocess\n",
    "import requests\n",
    "from io import StringIO\n",
    "from html.parser import HTMLParser\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s: %(message)s', datefmt='%m/%d/%Y %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c3126c-84a4-4603-a089-ffa365de6c6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "write_new_csv = False\n",
    "inplace = False\n",
    "scratch_path = os.environ.get('SCRATCH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288329ea-c88a-4636-95f9-a2c725d31427",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "simulation_dir = Path(scratch_path + '/cscr/') # Define your simulation directory\n",
    "if not os.path.exists(simulation_dir):\n",
    "    logging.warning(f'Simulation directory <{simulation_dir}> does not exist.')\n",
    "else:\n",
    "    logging.info(f'Found simulation directory <{simulation_dir}>.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33173da4-0666-40c4-8c4b-68b858583c8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_pkg_dir = Path(scratch_path + '/my_ATS_MDA/') # Define the destination directory for your ATS MDA\n",
    "try:\n",
    "    os.mkdir(data_pkg_dir)\n",
    "    logging.info(f'Data package directory <{data_pkg_dir}> created.')\n",
    "except FileExistsError:\n",
    "    logging.info(f'Data package directory <{data_pkg_dir}> exists.')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a1fcfd-5198-4cd0-a974-5cf4dae78717",
   "metadata": {},
   "source": [
    "# Make a copy of the data with certain extentions from `Simulation Directory` to `Data Package Directory`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af388c4-8607-44f3-8953-ad87bf05d496",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exts = ['exo', 'xml', 'csv', 'dat', 'txt', 'xmf', 'h5', 'out', 'nc', 'jpg', 'png', 'pdf'] # determine your own file types\n",
    "exts_to_include = ' '.join([f\"--include='*.{f}'\" for f in exts])\n",
    "exts_to_include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe0fbad-2f25-4413-ac7b-b58c3198476d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    subprocess.run(\"rsync -avzP --include=*/ \"+exts_to_include+f\" --include='slurm*' --exclude=* --prune-empty-dirs {simulation_dir}/ {data_pkg_dir}\", shell=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f'Command failed with exit code {e.returncode}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5d1961-2041-4bcc-aa63-48b6ce32e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find periodic checkpoints in run0 and run1\n",
    "run0_dir = subprocess.check_output([f'cd {data_pkg_dir}; find -name \"*run0*\"'], shell=True, encoding='utf-8').rstrip()\n",
    "run1_dir = subprocess.check_output([f'cd {data_pkg_dir}; find -name \"*run1*\"'], shell=True, encoding='utf-8').rstrip()\n",
    "run0_dir, run1_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dc64d6-d19d-4c09-9969-39d893d902e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run0_checkpoints = subprocess.check_output([f'cd {data_pkg_dir}; cd {run0_dir}; find -name \"checkpoint*.h5\"'], shell=True, encoding='utf-8').split('\\n')[1:-1]\n",
    "run1_checkpoints = subprocess.check_output([f'cd {data_pkg_dir}; cd {run1_dir}; find -name \"checkpoint*.h5\"'], shell=True, encoding='utf-8').split('\\n')[1:-1]\n",
    "run0_checkpoints, run1_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b922e085-5f50-4d93-a363-ecc15f9e6c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "run0_checkpoints_non_final, run1_checkpoints_non_final = [f for f in run0_checkpoints if 'final' not in f], [f for f in run1_checkpoints if 'final' not in f]\n",
    "run0_checkpoints_non_final, run1_checkpoints_non_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1044f5-f171-42a5-9b39-455015ac511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove periodic checkpoints in run0 and run1\n",
    "if len(run0_checkpoints_non_final) > 0:\n",
    "    for f in run0_checkpoints_non_final:\n",
    "        subprocess.run(f'cd {data_pkg_dir}; cd {run0_dir}; rm {f}', shell=True)\n",
    "        logging.info(f'Removed <{data_pkg_dir}{run0_dir}{f}> from your MDA.')\n",
    "else:\n",
    "    logging.info('Did not find any non-final checkpoints in <run0>')\n",
    "if len(run1_checkpoints_non_final) > 0:\n",
    "    for f in run1_checkpoints_non_final:\n",
    "        subprocess.run(f'cd {data_pkg_dir}; cd {run1_dir}; rm {f}', shell=True)\n",
    "        logging.info(f'Removed <{data_pkg_dir}{run0_dir}{f}> from your MDA.')\n",
    "else:\n",
    "    logging.info('Did not find any non-final checkpoints in <run1>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f4739-ce78-4cc1-9a4a-afe29c72fd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "run0_xmf = subprocess.check_output([f'cd {data_pkg_dir}; cd {run0_dir}; find -name \"*.xmf\"'], shell=True, encoding='utf-8').split('\\n')[1:-1]\n",
    "run1_xmf = subprocess.check_output([f'cd {data_pkg_dir}; cd {run1_dir}; find -name \"*.xmf\"'], shell=True, encoding='utf-8').split('\\n')[1:-1]\n",
    "run0_xmf, run1_xmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f155699d-fb6f-4cfb-b1e4-8a650849d7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove xmf files in run0 and run1\n",
    "if len(run0_xmf) > 0:\n",
    "    for f in run0_xmf:\n",
    "        subprocess.run(f'cd {data_pkg_dir}; cd {run0_dir}; rm {f}', shell=True)\n",
    "        logging.info(f'Removed <{f}> from your MDA.')\n",
    "else:\n",
    "    logging.info('Did not find any xmf files in <run0>')\n",
    "if len(run1_xmf) > 0:\n",
    "    for f in run1_xmf:\n",
    "        subprocess.run(f'cd {data_pkg_dir}; cd {run1_dir}; rm {f}', shell=True)\n",
    "        logging.info(f'Removed <{f}> from your MDA.')\n",
    "else:\n",
    "    logging.info('Did not find any xmf files in <run1>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14690fc-e64b-49e7-af8f-ac8c2494402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "run0_vis = subprocess.check_output([f'cd {data_pkg_dir}; cd {run0_dir}; find -name \"ats_vis_*.h5\"'], shell=True, encoding='utf-8').split('\\n')[1:-1]\n",
    "run1_vis = subprocess.check_output([f'cd {data_pkg_dir}; cd {run1_dir}; find -name \"ats_vis_*.h5\"'], shell=True, encoding='utf-8').split('\\n')[1:-1]\n",
    "run0_vis, run1_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74902b8-d726-40ab-ab34-5eae4cf6395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove visualization files in run0 and run1\n",
    "if len(run0_vis) > 0:\n",
    "    for f in run0_vis:\n",
    "        subprocess.run(f'cd {data_pkg_dir}; cd {run0_dir}; rm {f}', shell=True)\n",
    "        logging.info(f'Removed <{f}> from your MDA.')\n",
    "else:\n",
    "    logging.info('Did not find any visualization files in <run0>')\n",
    "if len(run1_vis) > 0:\n",
    "    for f in run1_vis:\n",
    "        subprocess.run(f'cd {data_pkg_dir}; cd {run1_dir}; rm {f}', shell=True)\n",
    "        logging.info(f'Removed <{f}> from your MDA.')\n",
    "else:\n",
    "    logging.info('Did not find any visualization files in <run1>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80637ba-8802-4eb7-9ae3-deaa835ff985",
   "metadata": {},
   "source": [
    "# Enumerate all files in this data package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2f51eb-3193-4b87-82b3-ddd3a94d46bc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    paths_and_files = sorted(subprocess.check_output([f'cd {data_pkg_dir}; find -name \"*.*\"'], shell=True, encoding='utf-8').split('\\n')[1:-1])\n",
    "    number_of_files = len(paths_and_files)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f'Command failed with exit code {e.returncode}')\n",
    "number_of_files, paths_and_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dbca25-8872-4ea6-9c11-a967fff3f8f0",
   "metadata": {},
   "source": [
    "# Parse paths and filenames (needed in `flmd.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b614775f-94ae-4dc7-ae0e-8a4e758a1ab5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "files, paths = [], []\n",
    "for path_and_file in paths_and_files:\n",
    "    files.append(path_and_file.split('/')[-1])\n",
    "    paths.append(path_and_file.replace(path_and_file.split('/')[-1], ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b396b257-e5c8-44e3-b16b-73ca3e321d53",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e73c2d-740a-4ac1-923e-f22e996c913c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32170b91-1675-4b84-b405-ba179753b6a1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "d = {'File_Name': files, \n",
    "     'File_Description': ['']*number_of_files, \n",
    "     'Standard': ['N/A']*number_of_files, \n",
    "     'Start_Date': [-9999]*number_of_files, \n",
    "     'End_Date': [-9999]*number_of_files, \n",
    "     'Missing_Value_Codes': ['N/A']*number_of_files, \n",
    "     'File_Path': paths}\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f3160-9f88-4407-8357-960b0f2f67d4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=d) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726728b4-7b24-48e1-a0fd-f01b3ce263ab",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "_dict = {'water_balance': 'Model output ascii file for the whole modeling domain.', \n",
    "         'checkpoint': 'Model output binary file as a checkpoint.',\n",
    "         'xml': 'Model configuration file.',\n",
    "         'xmf': 'Model Paraview visualization metadata file.',\n",
    "         'slurm': 'Model screen printout file.',\n",
    "         'pflotran': 'Model output file from the biogeochemical engine, Pflotran.',\n",
    "         'ats_vis': 'Model output binary file with all time frames.',\n",
    "         'LAI': 'Model input file (Leaf Area Index).',\n",
    "         'exo': 'Model input file (compuational mesh).'}\n",
    "_keys = list(_dict.keys())\n",
    "for k in range(len(_dict)):\n",
    "    df['File_Description'].iloc[[i if _keys[k] in df['File_Name'][i] else False for i in range(number_of_files)]] = _dict[_keys[k]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7653ebc1-9178-4f1c-8e04-49e0897ac75a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv(data_pkg_dir/'flmd.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1af9e66-6d2c-4a41-9c9b-b64204fdc811",
   "metadata": {},
   "source": [
    "# Find all csv files in this data package (needed in `dd.csv`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc9db4-e45e-4e8a-b3c0-89b54236cbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your observation file and file format\n",
    "obs_file_handle = 'water_balance'\n",
    "obs_file_format = 'csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51112be7-1ae0-465a-8fc8-b2ca4b1500c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "csv_paths_and_files = subprocess.check_output([f'cd {data_pkg_dir}; find -name \"{obs_file_handle}*.{obs_file_format}\"'], shell=True, encoding='utf-8').split('\\n')[1:-1]\n",
    "print(csv_paths_and_files[:10], f'... AND {len(csv_paths_and_files)} MORE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77aba818-0d91-4713-acf7-dfeac3e309ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv_file in csv_paths_and_files:\n",
    "    with open(data_pkg_dir/csv_file) as f:\n",
    "        lines = f.readlines()\n",
    "        if csv_file == csv_paths_and_files[0]:\n",
    "            print(lines[112])\n",
    "            print()\n",
    "            parameters = str([s.split(' [')[0] for s in lines[112].split(',')]).replace('\\'','').replace('\\\"','').replace(', ',',').replace(' ','_')[1:-1].split(',')\n",
    "            units = str([s.split(' [')[1] for s in lines[112].split(',')]).replace('\\'','').replace('\\\"','').replace(', ',',').replace(']','')[1:-2].split(',')\n",
    "            print(parameters)\n",
    "            print()\n",
    "            print(units)\n",
    "    if write_new_csv and inplace: # caution! inplace is dangerous!\n",
    "        with open(csv_file, 'w') as g:\n",
    "            g.write(''.join(lines))\n",
    "    elif write_new_csv:\n",
    "        with open(csv_file+'_tmp', 'w') as g:\n",
    "            g.write(''.join(lines))\n",
    "number_of_parameters, number_of_units = len(parameters), len(units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d6ba07-e97b-4afb-a49a-817eaabcf405",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f'Numer of parameters: {number_of_parameters}')\n",
    "logging.info(f'Numer of units: {number_of_units}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf8ff75-9417-4a71-853a-d9a7e99350af",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'Column_or_Row_Name': parameters, \n",
    "     'Unit': units, \n",
    "     'Definition': ['']*number_of_parameters, \n",
    "     'Data_Type': ['numeric']*number_of_parameters, \n",
    "     'Term_Type': ['column_header']*number_of_parameters}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5436b19-471f-47af-bd4b-959166b30781",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=d) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d11a835-ae06-4da1-ac65-e1d6a618fa69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML approach (deprecated)\n",
    "\n",
    "# class TableParser(HTMLParser):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.in_table = False\n",
    "#         self.in_tr = False\n",
    "#         self.in_th = False\n",
    "#         self.in_td = False\n",
    "#         self.headers = []\n",
    "#         self.current_row = []\n",
    "#         self.rows = []\n",
    "\n",
    "#     def handle_starttag(self, tag, attrs):\n",
    "#         if tag == \"table\":\n",
    "#             self.in_table = True\n",
    "#         elif tag == \"tr\" and self.in_table:\n",
    "#             self.in_tr = True\n",
    "#             self.current_row = []\n",
    "#         elif tag == \"th\" and self.in_tr:\n",
    "#             self.in_th = True\n",
    "#         elif tag == \"td\" and self.in_tr:\n",
    "#             self.in_td = True\n",
    "\n",
    "#     def handle_endtag(self, tag):\n",
    "#         if tag == \"table\":\n",
    "#             self.in_table = False\n",
    "#         elif tag == \"tr\":\n",
    "#             if self.in_tr:\n",
    "#                 if self.headers and len(self.current_row) == len(self.headers):\n",
    "#                     self.rows.append(dict(zip(self.headers, self.current_row)))\n",
    "#             self.in_tr = False\n",
    "#         elif tag == \"th\":\n",
    "#             self.in_th = False\n",
    "#         elif tag == \"td\":\n",
    "#             self.in_td = False\n",
    "\n",
    "#     def handle_data(self, data):\n",
    "#         if self.in_th:\n",
    "#             self.headers.append(data.strip())\n",
    "#         elif self.in_td:\n",
    "#             self.current_row.append(data.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0206ee48-0fd1-4236-abdd-70f571964f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ats_spec_url = \"https://amanzi.github.io/ats/stable/input_spec/introduction.html\"\n",
    "# response = requests.get(ats_spec_url)\n",
    "# parser = TableParser()\n",
    "# parser.feed(response.text)\n",
    "\n",
    "# result = {}\n",
    "# for row in parser.rows:\n",
    "#     if \"Variable Root Name\" in row and \"Description\" in row:\n",
    "#         result[row[\"Variable Root Name\"]] = row[\"Description\"]\n",
    "\n",
    "# logging.info(f'Numer of Variables found from ATS input spec: {len(result)}')\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8335f86-0deb-41ea-a363-2bf2c0253d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATS input spec symbol table approach\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/amanzi/ats/refs/heads/master/docs/documentation/source/input_spec/symbol_table.org\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "content = response.text\n",
    "\n",
    "table_lines = [line for line in content.splitlines() if line.strip().startswith('|')]\n",
    "table_text = \"\\n\".join(table_lines)\n",
    "\n",
    "df_input_spec = pd.read_csv(StringIO(table_text), sep='|', engine='python', skipinitialspace=True)\n",
    "df_input_spec = df_input_spec.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "df_input_spec = df_input_spec.loc[:, df_input_spec.columns.str.strip().astype(bool)]\n",
    "\n",
    "df_input_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd53d9d-f262-4b2d-886b-27996dd57eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_parameters_from_symbol_table = len(df_input_spec.iloc[1:, :])\n",
    "logging.info(f'Numer of parameters from the symbol table in ATS input spec: {number_of_parameters_from_symbol_table}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b629b5db-3983-4fed-b0ae-4cc3e9cfa153",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_dict = dict(zip(df_input_spec.iloc[1:, 1], df_input_spec.iloc[1:, 3]))\n",
    "_keys = list(_dict.keys())\n",
    "_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df83388e-512d-4213-8858-ced91260ddd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(len(_dict)):\n",
    "    df['Definition'].iloc[[i if _keys[k] in df['Column_or_Row_Name'][i] else False for i in range(len(df))]] = _dict[_keys[k]]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c2a3d5-8071-4fe8-9905-cb57d657973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(data_pkg_dir/'dd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f444a2-0c8b-467b-90a5-e669725537d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcec4657-34a5-47f2-9d0e-3e74c4237e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work in progress - read from xmf files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cdc5dd-3940-48e3-baf4-258886b0673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree = ET.parse('/pscratch/sd/l/lizh142/onedrive/DESK/o/surs/nach_sur/ats_vis_surface_data.h5.80000.xmf').getroot()\n",
    "# names = [attr.get(\"Name\") for attr in tree.findall(\".//Attribute\")]\n",
    "# print(names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
